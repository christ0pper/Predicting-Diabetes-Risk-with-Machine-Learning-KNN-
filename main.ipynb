# Install required packages if not already installed
if (!requireNamespace("class", quietly = TRUE)) install.packages("class")
if (!requireNamespace("caret", quietly = TRUE)) install.packages("caret")
if (!requireNamespace("ROCR", quietly = TRUE)) install.packages("ROCR")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")


# Now load the libraries
library(class)    # For KNN
library(caret)    # For confusion matrix and model evaluation
library(ROCR)     # For ROC curve
library(ggplot2)  # For visualization


# Set seed for reproducibility
set.seed(123)


# Load the local Pima Indians Diabetes dataset
file_path <- "/content/dataset.csv"
column_names <- c("Pregnancies", "Glucose", "BloodPressure", "SkinThickness",
                  "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome")
diabetes <- read.csv(file_path, header = FALSE)
names(diabetes) <- column_names


# Explore the data
str(diabetes)
summary(diabetes)


# Check for missing values (represented as 0 in this dataset)
sapply(diabetes, function(x) sum(x == 0))


# Replace 0's with NA for columns where 0 is not a valid value
diabetes$Glucose[diabetes$Glucose == 0] <- NA
diabetes$BloodPressure[diabetes$BloodPressure == 0] <- NA
diabetes$SkinThickness[diabetes$SkinThickness == 0] <- NA
diabetes$Insulin[diabetes$Insulin == 0] <- NA
diabetes$BMI[diabetes$BMI == 0] <- NA


# Impute missing values with median
for(i in 1:ncol(diabetes)) {
  if(any(is.na(diabetes[,i]))) {
    diabetes[is.na(diabetes[,i]), i] <- median(diabetes[,i], na.rm = TRUE)
  }
}


# Check class distribution
table(diabetes$Outcome)
prop.table(table(diabetes$Outcome))


# Convert outcome to factor
diabetes$Outcome <- factor(diabetes$Outcome, levels = c(0, 1), labels = c("Negative", "Positive"))


# Split data into training and test sets (70/30 split)
train_indices <- createDataPartition(diabetes$Outcome, p = 0.7, list = FALSE)
train_data <- diabetes[train_indices, ]
test_data <- diabetes[-train_indices, ]


# Scale the data (important for KNN)
# Get only the predictor variables (exclude the outcome)
train_predictors <- train_data[, 1:8]
test_predictors <- test_data[, 1:8]
train_outcome <- train_data$Outcome
test_outcome <- test_data$Outcome


# Scale the data
preproc <- preProcess(train_predictors, method = c("center", "scale"))
train_predictors_scaled <- predict(preproc, train_predictors)
test_predictors_scaled <- predict(preproc, test_predictors)


# Find optimal K value
k_values <- seq(1, 25, by = 2)
accuracies <- numeric(length(k_values))


for (i in seq_along(k_values)) {
  knn_pred <- knn(train = train_predictors_scaled,
                  test = test_predictors_scaled,
                  cl = train_outcome,
                  k = k_values[i])
 
  cm <- confusionMatrix(knn_pred, test_outcome, positive = "Positive")
  accuracies[i] <- cm$overall["Accuracy"]
}


# Find the optimal k value
best_k <- k_values[which.max(accuracies)]
cat("Best K value:", best_k, "\n")


# Plot accuracies vs K values
k_results <- data.frame(K = k_values, Accuracy = accuracies)
ggplot(k_results, aes(x = K, y = Accuracy)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "Accuracy vs K Values",
       x = "K Value",
       y = "Accuracy") +
  theme_minimal() +
  geom_vline(xintercept = best_k, linetype = "dashed", color = "darkred")


# Build KNN model with the best K value
knn_pred <- knn(train = train_predictors_scaled,
                test = test_predictors_scaled,
                cl = train_outcome,
                k = best_k,
                prob = TRUE)


# Get probabilities
knn_probs <- attr(knn_pred, "prob")
# Adjust probabilities to ensure they correspond to the "Positive" class
knn_probs <- ifelse(knn_pred == "Positive", knn_probs, 1 - knn_probs)


# Evaluate the model
cm <- confusionMatrix(knn_pred, test_outcome, positive = "Positive")
print(cm)


# Calculate ROC curve
pred_obj <- prediction(knn_probs, test_outcome)
perf <- performance(pred_obj, "tpr", "fpr")
auc <- performance(pred_obj, "auc")@y.values[[1]]


# Plot ROC curve
plot(perf, main = "ROC Curve for KNN Classifier",
     col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
text(0.6, 0.2, paste("AUC =", round(auc, 3)))


# Feature importance analysis
# For KNN, we can use a permutation-based approach
# where we permute each feature and see how much the accuracy decreases


# Create empty dataframe to store importance scores
feature_importance <- data.frame(
  Feature = character(0),
  Importance = numeric(0)
)


# Calculate baseline accuracy
baseline_knn <- knn(train = train_predictors_scaled,
                    test = test_predictors_scaled,
                    cl = train_outcome,
                    k = best_k)
baseline_acc <- mean(baseline_knn == test_outcome)


# Calculate importance for each feature
for (feature in names(train_predictors)) {
  # Create a permuted version of the test data
  permuted_test <- test_predictors_scaled
  permuted_test[, feature] <- sample(permuted_test[, feature])
 
  # Predict with permuted feature
  permuted_pred <- knn(train = train_predictors_scaled,
                       test = permuted_test,
                       cl = train_outcome,
                       k = best_k)
 
  # Calculate accuracy drop
  permuted_acc <- mean(permuted_pred == test_outcome)
  importance <- baseline_acc - permuted_acc
 
  # Add to the feature importance dataframe
  feature_importance <- rbind(feature_importance,
                             data.frame(Feature = feature, Importance = importance))
}


# Order by importance
feature_importance <- feature_importance[order(feature_importance$Importance, decreasing = TRUE), ]


# Visualize feature importance
ggplot(feature_importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature Importance in KNN Model",
       x = "Feature",
       y = "Decrease in Accuracy when Feature is Permuted")


# Print the ordered features by importance
print(feature_importance)


# Create a summary of results
cat("\n----- MODEL EVALUATION SUMMARY -----\n")
cat("Accuracy:", round(cm$overall["Accuracy"] * 100, 1), "%\n")
cat("Sensitivity (True Positive Rate):", round(cm$byClass["Sensitivity"] * 100, 1), "%\n")
cat("Specificity (True Negative Rate):", round(cm$byClass["Specificity"] * 100, 1), "%\n")
cat("Positive Predictive Value:", round(cm$byClass["Pos Pred Value"] * 100, 1), "%\n")
cat("Negative Predictive Value:", round(cm$byClass["Neg Pred Value"] * 100, 1), "%\n")
cat("Area Under ROC Curve (AUC):", round(auc, 3), "\n")
cat("\n----- TOP PREDICTIVE FEATURES -----\n")
print(feature_importance)
